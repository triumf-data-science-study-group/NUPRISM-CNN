{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Study Group Project: Neutrino type and background classification\n",
    "# Data Handling Tutorial "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "I will assume everybody here is roughly familiar with physics of neutrinos and Water Cherenkov detectors.\n",
    "In this project we will tackle the task of classification of neutrino type ($\\nu_e$ or $\\nu_\\mu$) or rather the charged leptons resulting from the nuclear scatter ($e$ and  $\\mu$) as well as an irreducible background from neutral current $\\gamma$ production. The dataset comes from simulated Water Cherenkov detector called NuPRISM. NuPRISM is a proposed 'intermediate' detector for the Hyper-Kamiokande project. The detector has a cylindrical geometry and can be lowered and raised in a shaft to sample different energy distribution of incoming neutrinos! ![NUPRISM](img/NUPRISM_diag.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cylinder wall or 'barrel' and end-caps are lined with 'multi-PMT' or 'mPMT' modules arranged in a rectangular grid. Each mPMT is a dome with 19 PMTs arranged in two rings and one at the center:![mPMT](img/mPMT.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an event display where the barrel was 'unrolled':\n",
    "![eventdisp](img/ev_disp.png) - you can clearly see a Cherenkov ring appearing\n",
    "The 'brightness' corresponds to charge collected by each PMT. Each PMT also tells us the arrival time of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first tutorial we will take a look at the data and how to organize streaming it in batches so that we can feed it to our CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up on cedar.computecanada.ca\n",
    "We will be working on cedar - a Compute Canada cluster located at SFU.\n",
    "We will use a container prepared with all the software we might need - and that will also enable us to use GPUs later on.\n",
    "Here are instructions on how to get started. The istructions below should work for Mac, linux, and Windows 10 Windows Linux Subsystem (Ubuntu bash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If you don't have a github account [MAKE ONE NOW](https://github.com/join)\n",
    "1. If you don't have a ssh keys from cedar linked to the github account make them now:\n",
    "   1. If you have no ~/.ssh directory or there is no id_rsa.pub file in that directory type: `ssh -t rsa -b 4096` - and press `Enter` for all prompts\n",
    "   1. do `cat ~/.ssh/id_rsa.pub` and copy the output to clipboard\n",
    "   1. on github website click on your avatar in upper right corner, chooose 'Settings'\n",
    "   1. From menu on the left click on 'SSH and GPG keys'\n",
    "   1. Click on 'New SSH key' on top right\n",
    "   1. Paste entire clipboard into the 'Key' field; Name it something snsible e.g. 'cedar'\n",
    "   \n",
    "1. First log in to cedar and go to a directory where you can submit jobs from e.g. your /project space then clone the repository where this code lives:  \n",
    "`git clone git@github.com:triumf-data-science-study-group/NUPRISM-CNN.git`\n",
    "1. cd inside. There are a few scripts in the repository: `jupyter_job.sh`, `enter_container.sh` and `start_jupyternotebook.sh` 1. these in order are:\n",
    "   1. SLURM job submission script\n",
    "   1. script that launches a container\n",
    "   1. script that launches a jupyter notebook within a container\n",
    "1. We will be using container software called [singularity](https://www.sylabs.io/). Singularity can run docker containers but is preferred on HPC clusters due to security concerns.\n",
    "1. The Docker container is based on Nvidia GPU Cloud ([NGC](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_19-04.html#rel_19-04)) container. The Dockerfile with the recipe is [here](https://github.com/triumfmlutils/Container_Base_ML) and image is hosted on docker hub [here](https://cloud.docker.com/u/triumfmlutils/repository/docker/triumfmlutils/baseml) - you are welcome to pull/extend, but to make things simpler I pulled it onto cedar for you already.\n",
    "1. Go ahead and examine the scripts...\n",
    "1. Ok let's submit a job that will launch a jupyter server for you:  \n",
    "`sbatch ./jupyter_job.sh`\n",
    "NOTE: if you have multiple roles/projects you will need to specifiy the 'account' under which to submit the job. Generally don't use a project allocation account - that would deplete the allocation. Instead use the 'default' account - this will usually be `def-your-PI-last-name` for example: `sbatch --account=def-doe ./jupyter_job.sh`\n",
    "1. Hopefully fairly quickly you will get a job running - to sheck if the job is running type:  \n",
    "`squeue -u ${USER}`\n",
    "1. You should see something resempling this:  \n",
    "`   JOBID     USER      ACCOUNT           NAME  ST START_TIME        TIME_LEFT NODES CPUS       GRES MIN_MEM NODELIST (REASON)              21345415 wfedorko def-tafirout       notebook   R 2019-05-22T02:07    4:19:09     1    4     (null)  16000M cdr693 (None) `\n",
    "1. The `ST`atus column `R` means job is now running `PD` means its pending in the queue.\n",
    "1. Once the job is running do `ls -rt1` and last file should be something like `log-jupyter-wfedorko-21345415.txt` - except with your userid and job number. Go ahead and less that file. If you have `sshuttle` on a linux machine or a Mac you can use the printed instructions and connect to a notebook. `sshuttle` will not work on Windows 10 Ubuntu bash shell. On all systems you can follow these intructions to open an ssh tunnel and bring up jupyter notebook:\n",
    "   1. In the log file jupyter told us to connect to the host at a specific port. e.g. in my logfile I see a line toward the end telling me to paste this into my browser: `http://cdr693.int.cedar.computecanada.ca:8888/?token=alotofhexdigitswithtokennumber` - but we can not 'see' the compute node `cdr693` from 'outside' cedar. So let's set up an ssh tunnel via cedar. In this case I did this in a separate shell (on my laptop):  \n",
    "   `ssh -L 8888:cdr693.int.cedar.computecanada.ca:8888 mycomputecanadauser@cedar.computecanada.ca -N -f`; Then in my browser I paste what jupyter told me to - but I replace the `cdr...canada.ca` with `localhost` - in this case:  \n",
    "   `http://localhost:8888/?token=alotofhexdigitswithtokennumber` - you shoud get a jupyter 'tree' window and be able to re-open this notebok under `Data Handling Tutorial.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! now that you are back lets see if we can see the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 wfedorko wfedorko 82G May 22 17:15 /scratch/wfedorko/TRIUMF_DS_NUPRISM/merged_IWCDmPMT_varyE.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/wfedorko/TRIUMF_DS_NUPRISM/merged_IWCDmPMT_varyE.h5 -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly look what's inside... - import h5py, numpy etc and open for reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, time\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File(\"/scratch/wfedorko/TRIUMF_DS_NUPRISM/merged_IWCDmPMT_varyE.h5\",\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keys()` will give us all the hdf5 datasets stored in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['event_data', 'labels']>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shapes of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 16, 40, 38)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['event_data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 900k simulated scattering events here! labels are 0, 1, 2 for $\\gamma$,$e$ and $\\mu$ respectively. The 'event_data' contains only the barrel portion of the tank which has been 'unrolled'. The first dimension (900k) enumerates over the events, the second two dimensions (16,40) enumerate over the row and column in the module 'grid'. Finally last two dimensions enumerate over the PMT within the module (again there are 19 in each mPMT module) first 19 entries correspond to charge collected on a given PMT and last 19 correspond to the time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import and create a Dataset object - you are welcome to look at the [source](http://localhost:8888/edit/io_utils/data_handling.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io_utils.data_handling import WCH5Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class derives from the torch Dataset object. The two heavy lifters are the __init__ function and __getitem__ function.\n",
    "Let's look at what init does:  \n",
    "       \n",
    "       `f=h5py.File(path,'r')\n",
    "        hdf5_event_data = f[\"event_data\"]\n",
    "        hdf5_labels=f[\"labels\"]\n",
    "\n",
    "        assert hdf5_event_data.shape[0] == hdf5_labels.shape[0]\n",
    "\n",
    "        event_data_shape = hdf5_event_data.shape\n",
    "        event_data_offset = hdf5_event_data.id.get_offset()\n",
    "        event_data_dtype = hdf5_event_data.dtype\n",
    "        \n",
    "        labels_shape = hdf5_labels.shape\n",
    "        labels_offset = hdf5_labels.id.get_offset()\n",
    "        labels_dtype = hdf5_labels.dtype`  \n",
    "-here we opened the file and got the offsets, shapes, and data types of the datasets. Why are we doing this? This is because the hdf5 file is uncompressed and the datasets within are contiguous - this allows us to do memory mapping of the file. This is important with large datasets like this - where we are most like not going to be able to load everything into memory. Withh memory map we can only load what we need, when we need it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory map itself  \n",
    "\n",
    "        `self.event_data = np.memmap(path, mode='r', shape=event_data_shape, offset=event_data_offset, dtype=event_data_dtype)`\n",
    "For the labels - we just load them into memory bu constructing a numpy array - this should be no problem even for very large datasets  \n",
    "\n",
    "        `self.labels = np.array(hdf5_labels)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of __init__ function computes indices for training, validation and testing sets, based on a random (but consistent) shuffle of events - this will be useful later when trainnig our model. We also provide a facility to only use a subset of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have the __getitem__ method - this provides functionality for the subscript [] operator. Only here we actually load the event_data that was requested:  \n",
    "\n",
    "            `return np.array(self.event_data[index,:]),  self.labels[index]`\n",
    "-we return a tuple with two elements - first is the event 'image' and second is the label. If you look at the code you will notice that there is also a provision for providing a transform (which will not do anything atm). This is very useful if you want to do data augmentation on-the-fly. E.g. we could flip the images to 'populate' the dataset to reflect the variability we expect in the dataset we will want to apply the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally we need the len method - this just needs to return how many exmples we have in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's instantiate the dataset and ask it for a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset=WCH5Dataset(\"/scratch/wfedorko/TRIUMF_DS_NUPRISM/merged_IWCDmPMT_varyE.h5\",val_split=0.1,test_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import matplotlib and tell it to plot inline in the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some random event and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "event, label=dset[800001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to plot only the PMT charge for the 'center' PMT in mPMT modules - i believe this is at channel 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class is 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8),facecolor='w')\n",
    "plt.imshow(event[:,:,18],cmap='jet',origin='lower')\n",
    "ax.set_title('Event Data',fontsize=20,fontweight='bold')\n",
    "print('class is {}'.format(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader objects and streaming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets actually create DataLoader objects - one for each training, validation and testing set - but each DataLoader uses the same dataset -that way we keep only one open file (this is the 'standard' pytorch way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "train_iter=DataLoader(dset,batch_size=64,shuffle=False,sampler=SubsetRandomSampler(dset.train_indices),num_workers=4)\n",
    "val_iter=DataLoader(dset,batch_size=64,shuffle=False,sampler=SubsetRandomSampler(dset.val_indices),num_workers=4)\n",
    "test_iter=DataLoader(dset,batch_size=64,shuffle=False,sampler=SubsetRandomSampler(dset.test_indices),num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see the parameters - like batch_size and sampler - the sampler uses the indices we computed for the training, validation and testing set - if you use a sampler shuffle has to be False. On each iteration the DataLoader object will ask the dataset for a bunch of indices (calling the __getitem__ function we coded earlier) and then collate the data into a batch tensor. The collating can also be customized by providing collate_fn - but for now we will leave it with a default behavior. Did you notice the `num_workers` argument? if >0 this will enable multiprocessing - several processes will be reading examples (if supplied applying the augmentation transformation) and putting the data on queue that would be than 'consumed' by your training/evaluation process. We requested 4 CPUs for the job so we will use that. We are beating on the same storage with all threads - so if we aren't doing much preprocessing it doesn't make sense to make this too high..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you would iterate over several batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_over_set(loader,loop_limit=3):\n",
    "\n",
    "    # Let's measure time that takes in each loop\n",
    "    trecord = np.zeros([loop_limit],dtype=np.float32)\n",
    "    t = time.time()\n",
    "    for iteration, batch in enumerate(loader):\n",
    "\n",
    "        data,label = batch\n",
    "\n",
    "        # Print out some content info\n",
    "        print('Iteration',iteration,'... time:',time.time()-t,'[s]')\n",
    "        print('    Labels:',label)\n",
    "\n",
    "        trecord[iteration] = time.time() - t\n",
    "        t = time.time()\n",
    "\n",
    "        # break when reaching the loop limit\n",
    "        if (iteration+1) == loop_limit:\n",
    "            break\n",
    "    return trecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convince yourself that the `data` and `label` are already tensors - which we could plug into our (future) model. Now let's iterate over first 40 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 ... time: 13.395081520080566 [s]\n",
      "    Labels: tensor([0., 1., 0., 0., 0., 2., 0., 0., 1., 1., 2., 2., 1., 2., 1., 1., 1., 0.,\n",
      "        2., 0., 1., 0., 2., 0., 2., 0., 0., 0., 2., 0., 1., 2., 2., 1., 2., 2.,\n",
      "        2., 2., 0., 1., 2., 2., 2., 1., 2., 0., 1., 1., 0., 1., 2., 1., 2., 2.,\n",
      "        0., 2., 1., 2., 2., 1., 2., 2., 0., 0.])\n",
      "Iteration 1 ... time: 0.1952211856842041 [s]\n",
      "    Labels: tensor([1., 1., 0., 1., 2., 2., 1., 2., 0., 0., 0., 0., 1., 2., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 2., 0., 2., 0., 2., 2., 0., 0., 1., 2., 0.,\n",
      "        1., 1., 2., 1., 1., 2., 2., 1., 1., 1., 2., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 2., 1., 2., 0., 0., 1., 2., 2., 0.])\n",
      "Iteration 2 ... time: 0.0013921260833740234 [s]\n",
      "    Labels: tensor([2., 1., 0., 2., 1., 1., 0., 2., 0., 1., 1., 0., 2., 0., 1., 1., 1., 0.,\n",
      "        0., 2., 2., 1., 0., 2., 0., 1., 0., 1., 0., 1., 1., 2., 2., 1., 1., 2.,\n",
      "        1., 1., 0., 0., 1., 1., 2., 1., 1., 0., 2., 0., 1., 2., 0., 2., 2., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 2.])\n",
      "Iteration 3 ... time: 0.0013928413391113281 [s]\n",
      "    Labels: tensor([0., 1., 0., 1., 2., 1., 2., 2., 1., 0., 0., 0., 2., 2., 2., 0., 2., 2.,\n",
      "        0., 1., 1., 2., 1., 1., 1., 2., 0., 0., 1., 0., 2., 1., 2., 2., 1., 2.,\n",
      "        0., 2., 0., 1., 2., 2., 0., 1., 0., 0., 2., 2., 2., 0., 1., 2., 1., 0.,\n",
      "        0., 0., 2., 1., 1., 1., 1., 0., 2., 0.])\n",
      "Iteration 4 ... time: 14.181246519088745 [s]\n",
      "    Labels: tensor([0., 1., 2., 2., 1., 2., 0., 0., 1., 1., 1., 2., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 2., 1., 0., 0., 0., 2., 2., 0., 0., 2., 2., 1., 2., 2., 1.,\n",
      "        1., 0., 1., 1., 2., 0., 0., 0., 1., 0., 1., 1., 2., 1., 0., 0., 2., 0.,\n",
      "        0., 1., 0., 0., 1., 2., 0., 1., 2., 2.])\n",
      "Iteration 5 ... time: 1.9495925903320312 [s]\n",
      "    Labels: tensor([0., 1., 0., 1., 0., 2., 1., 2., 1., 2., 2., 1., 0., 2., 1., 2., 2., 1.,\n",
      "        1., 2., 1., 2., 1., 2., 2., 1., 0., 0., 0., 0., 1., 2., 1., 2., 0., 1.,\n",
      "        2., 1., 0., 1., 2., 1., 2., 0., 1., 1., 1., 1., 0., 2., 2., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 2., 0., 2., 0., 0.])\n",
      "Iteration 6 ... time: 0.002166748046875 [s]\n",
      "    Labels: tensor([1., 0., 1., 1., 1., 2., 1., 2., 0., 0., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
      "        0., 0., 2., 0., 0., 2., 1., 0., 1., 0., 2., 1., 1., 1., 2., 1., 1., 2.,\n",
      "        1., 1., 0., 0., 0., 2., 0., 0., 1., 2., 0., 1., 2., 0., 1., 0., 2., 0.,\n",
      "        0., 0., 2., 0., 2., 1., 0., 2., 2., 0.])\n",
      "Iteration 7 ... time: 0.0020902156829833984 [s]\n",
      "    Labels: tensor([2., 0., 0., 0., 0., 2., 1., 0., 2., 2., 0., 0., 0., 1., 2., 1., 1., 2.,\n",
      "        1., 2., 2., 2., 1., 0., 0., 2., 0., 1., 0., 1., 2., 1., 0., 2., 0., 2.,\n",
      "        2., 1., 1., 0., 1., 0., 0., 1., 1., 1., 2., 0., 0., 2., 2., 0., 2., 0.,\n",
      "        0., 1., 0., 2., 1., 2., 0., 0., 1., 1.])\n",
      "Iteration 8 ... time: 11.347413539886475 [s]\n",
      "    Labels: tensor([2., 2., 2., 0., 1., 1., 0., 2., 0., 0., 1., 2., 2., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 2., 2., 0., 2., 2., 2., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        2., 0., 0., 2., 1., 2., 2., 2., 1., 1., 2., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 2., 2., 0., 0., 0., 2., 2., 0., 2.])\n",
      "Iteration 9 ... time: 1.9182608127593994 [s]\n",
      "    Labels: tensor([1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 2., 2., 0., 1., 2., 1., 1., 2.,\n",
      "        0., 2., 2., 2., 2., 0., 2., 0., 0., 2., 2., 0., 1., 2., 2., 2., 0., 2.,\n",
      "        2., 2., 1., 2., 0., 0., 1., 2., 2., 0., 1., 2., 0., 1., 2., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 2., 0., 1., 1., 2.])\n",
      "Iteration 10 ... time: 0.0013265609741210938 [s]\n",
      "    Labels: tensor([2., 2., 2., 2., 1., 0., 1., 1., 1., 0., 0., 2., 2., 1., 1., 2., 2., 0.,\n",
      "        1., 0., 1., 1., 2., 2., 1., 0., 2., 2., 2., 1., 2., 2., 0., 1., 1., 1.,\n",
      "        1., 0., 2., 2., 2., 1., 0., 0., 1., 0., 2., 1., 1., 1., 2., 2., 1., 1.,\n",
      "        0., 0., 1., 2., 0., 1., 1., 1., 2., 2.])\n",
      "Iteration 11 ... time: 0.0013117790222167969 [s]\n",
      "    Labels: tensor([0., 0., 1., 1., 1., 0., 0., 1., 0., 2., 0., 2., 2., 1., 2., 0., 2., 0.,\n",
      "        0., 2., 0., 2., 0., 1., 1., 2., 0., 0., 0., 0., 0., 1., 0., 0., 2., 0.,\n",
      "        1., 1., 1., 1., 0., 2., 1., 1., 0., 2., 2., 2., 2., 2., 1., 2., 1., 2.,\n",
      "        2., 0., 1., 0., 0., 1., 0., 2., 2., 1.])\n",
      "Iteration 12 ... time: 8.565473556518555 [s]\n",
      "    Labels: tensor([0., 0., 2., 2., 1., 1., 0., 1., 2., 2., 1., 2., 1., 0., 2., 2., 0., 2.,\n",
      "        0., 1., 0., 1., 2., 0., 1., 2., 1., 0., 0., 1., 2., 2., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 2., 0., 1., 2., 0., 0., 0., 1., 0., 2.,\n",
      "        0., 0., 0., 1., 0., 0., 2., 2., 2., 2.])\n",
      "Iteration 13 ... time: 4.606346607208252 [s]\n",
      "    Labels: tensor([2., 0., 2., 1., 1., 2., 1., 1., 1., 0., 1., 2., 2., 1., 1., 0., 1., 2.,\n",
      "        2., 0., 2., 2., 2., 1., 2., 2., 1., 2., 0., 1., 1., 2., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 2., 1., 0., 1., 2., 1., 2., 1., 2., 2., 2.,\n",
      "        2., 2., 1., 0., 0., 1., 1., 0., 2., 1.])\n",
      "Iteration 14 ... time: 0.0015039443969726562 [s]\n",
      "    Labels: tensor([0., 0., 2., 2., 2., 1., 2., 2., 0., 2., 2., 0., 0., 1., 1., 0., 2., 1.,\n",
      "        1., 0., 2., 2., 1., 1., 2., 2., 1., 0., 0., 2., 2., 2., 1., 1., 1., 1.,\n",
      "        1., 0., 2., 0., 1., 0., 1., 0., 0., 1., 0., 2., 1., 2., 0., 2., 1., 0.,\n",
      "        0., 2., 1., 2., 2., 1., 2., 0., 2., 0.])\n",
      "Iteration 15 ... time: 0.001523733139038086 [s]\n",
      "    Labels: tensor([2., 0., 0., 2., 1., 1., 1., 0., 2., 2., 2., 2., 1., 1., 0., 1., 1., 0.,\n",
      "        2., 1., 2., 0., 0., 1., 1., 0., 2., 2., 1., 1., 2., 0., 0., 2., 2., 1.,\n",
      "        2., 0., 1., 1., 0., 0., 1., 0., 0., 2., 0., 2., 2., 2., 1., 2., 1., 1.,\n",
      "        0., 1., 2., 1., 0., 0., 2., 0., 1., 0.])\n",
      "Iteration 16 ... time: 7.5605149269104 [s]\n",
      "    Labels: tensor([2., 0., 0., 1., 0., 0., 0., 2., 1., 0., 1., 1., 2., 0., 2., 1., 1., 2.,\n",
      "        1., 0., 0., 2., 2., 0., 1., 1., 1., 2., 1., 2., 1., 1., 2., 2., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 2., 2., 1., 1., 2., 2., 1., 2., 2., 1., 2., 0.,\n",
      "        1., 2., 2., 2., 2., 1., 2., 1., 2., 2.])\n",
      "Iteration 17 ... time: 4.9889843463897705 [s]\n",
      "    Labels: tensor([2., 2., 2., 2., 2., 1., 2., 2., 1., 1., 2., 0., 0., 1., 0., 2., 1., 2.,\n",
      "        1., 0., 1., 0., 0., 1., 2., 0., 2., 1., 1., 1., 2., 1., 2., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 2., 2., 0., 2., 1., 0., 0., 1., 0., 1., 2., 2., 2., 0.,\n",
      "        1., 0., 0., 2., 2., 2., 2., 2., 0., 2.])\n",
      "Iteration 18 ... time: 0.001558065414428711 [s]\n",
      "    Labels: tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 2., 2., 1., 0., 0., 0., 2., 0., 2.,\n",
      "        1., 0., 1., 2., 1., 0., 0., 2., 2., 2., 1., 1., 0., 1., 2., 2., 0., 2.,\n",
      "        1., 0., 0., 2., 1., 2., 0., 0., 2., 2., 0., 2., 2., 0., 2., 0., 2., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 2., 1., 1., 0.])\n",
      "Iteration 19 ... time: 0.001689910888671875 [s]\n",
      "    Labels: tensor([2., 1., 2., 1., 1., 1., 2., 1., 2., 2., 0., 1., 2., 2., 2., 0., 0., 2.,\n",
      "        2., 1., 1., 2., 1., 0., 2., 2., 1., 1., 1., 1., 0., 1., 2., 1., 0., 1.,\n",
      "        2., 2., 1., 1., 1., 2., 0., 2., 2., 2., 1., 0., 2., 1., 2., 1., 1., 2.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 2., 0., 2.])\n",
      "Iteration 20 ... time: 8.092904090881348 [s]\n",
      "    Labels: tensor([1., 1., 2., 1., 2., 0., 0., 0., 0., 2., 1., 1., 1., 1., 1., 0., 1., 2.,\n",
      "        0., 2., 1., 2., 1., 1., 2., 1., 1., 2., 1., 0., 0., 0., 0., 2., 0., 1.,\n",
      "        1., 1., 0., 0., 2., 1., 1., 1., 1., 1., 0., 2., 0., 2., 0., 1., 2., 0.,\n",
      "        2., 1., 0., 2., 1., 2., 0., 1., 2., 1.])\n",
      "Iteration 21 ... time: 3.2707839012145996 [s]\n",
      "    Labels: tensor([1., 1., 2., 0., 2., 2., 0., 0., 2., 0., 0., 1., 1., 1., 0., 2., 1., 0.,\n",
      "        0., 2., 1., 1., 2., 2., 1., 0., 2., 2., 2., 0., 1., 0., 1., 2., 2., 1.,\n",
      "        2., 0., 1., 1., 2., 0., 2., 1., 0., 0., 0., 1., 2., 1., 0., 2., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 2., 2., 1., 1.])\n",
      "Iteration 22 ... time: 0.0019116401672363281 [s]\n",
      "    Labels: tensor([1., 1., 1., 2., 2., 2., 2., 0., 0., 1., 2., 2., 2., 0., 2., 0., 1., 2.,\n",
      "        0., 0., 0., 1., 2., 1., 2., 2., 1., 2., 0., 2., 0., 2., 1., 2., 0., 2.,\n",
      "        0., 0., 2., 0., 0., 1., 2., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 2., 1., 0.])\n",
      "Iteration 23 ... time: 0.0019397735595703125 [s]\n",
      "    Labels: tensor([1., 2., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 2., 0., 0., 1., 0.,\n",
      "        2., 2., 2., 2., 0., 2., 2., 1., 1., 0., 0., 0., 0., 0., 2., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 1., 0., 2., 0., 2., 0., 0., 2., 0., 0., 1.,\n",
      "        2., 1., 2., 0., 0., 0., 2., 0., 2., 2.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24 ... time: 10.637712240219116 [s]\n",
      "    Labels: tensor([0., 2., 0., 0., 2., 0., 0., 1., 2., 1., 0., 1., 2., 0., 0., 1., 2., 2.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 2., 0., 2., 1., 0., 2., 1., 0.,\n",
      "        2., 2., 2., 1., 1., 2., 0., 0., 1., 1., 1., 2., 1., 2., 2., 1., 2., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 2., 1., 1., 1.])\n",
      "Iteration 25 ... time: 2.0094218254089355 [s]\n",
      "    Labels: tensor([2., 2., 2., 0., 2., 0., 1., 1., 0., 1., 1., 2., 2., 0., 2., 1., 1., 2.,\n",
      "        1., 2., 0., 0., 1., 1., 1., 1., 2., 2., 0., 1., 0., 2., 0., 0., 0., 2.,\n",
      "        0., 2., 2., 0., 0., 2., 2., 0., 0., 0., 0., 2., 2., 0., 2., 2., 0., 0.,\n",
      "        0., 0., 2., 0., 0., 2., 2., 2., 0., 0.])\n",
      "Iteration 26 ... time: 0.001748800277709961 [s]\n",
      "    Labels: tensor([1., 1., 0., 2., 1., 1., 0., 2., 0., 1., 0., 1., 0., 0., 0., 2., 0., 0.,\n",
      "        2., 2., 1., 0., 2., 1., 0., 1., 1., 1., 2., 0., 2., 1., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 2., 1., 0., 1., 2., 0., 1., 0., 0., 2., 2., 2., 1., 1., 2.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])\n",
      "Iteration 27 ... time: 0.0017518997192382812 [s]\n",
      "    Labels: tensor([2., 0., 1., 2., 0., 2., 0., 0., 0., 1., 1., 2., 1., 0., 2., 0., 0., 0.,\n",
      "        0., 1., 2., 0., 0., 1., 0., 2., 0., 2., 0., 2., 0., 1., 1., 2., 1., 1.,\n",
      "        1., 1., 2., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 1., 1., 2., 0.,\n",
      "        1., 0., 1., 1., 2., 1., 2., 1., 2., 1.])\n",
      "Iteration 28 ... time: 9.576932191848755 [s]\n",
      "    Labels: tensor([0., 1., 0., 2., 0., 0., 1., 0., 1., 1., 0., 2., 1., 2., 2., 2., 2., 2.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 2., 2.,\n",
      "        1., 0., 1., 1., 2., 2., 1., 0., 1., 0., 1., 0., 1., 0., 1., 2., 2., 1.,\n",
      "        0., 2., 1., 0., 2., 0., 2., 1., 2., 2.])\n",
      "Iteration 29 ... time: 7.714834928512573 [s]\n",
      "    Labels: tensor([0., 1., 0., 0., 2., 0., 0., 2., 1., 1., 0., 0., 2., 0., 1., 1., 2., 1.,\n",
      "        2., 1., 2., 0., 2., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 2., 2.,\n",
      "        0., 0., 1., 1., 2., 2., 1., 0., 1., 0., 2., 0., 1., 0., 1., 2., 1., 1.,\n",
      "        0., 2., 1., 1., 0., 2., 2., 0., 0., 1.])\n",
      "Iteration 30 ... time: 0.002009153366088867 [s]\n",
      "    Labels: tensor([0., 2., 0., 2., 1., 1., 1., 0., 1., 1., 2., 2., 0., 0., 2., 2., 1., 0.,\n",
      "        2., 1., 2., 1., 2., 0., 0., 0., 1., 2., 0., 0., 1., 2., 1., 0., 1., 2.,\n",
      "        0., 0., 2., 0., 1., 2., 2., 2., 0., 2., 0., 0., 0., 2., 1., 2., 0., 1.,\n",
      "        1., 2., 0., 2., 2., 2., 1., 0., 2., 0.])\n",
      "Iteration 31 ... time: 0.0018389225006103516 [s]\n",
      "    Labels: tensor([1., 0., 0., 0., 0., 2., 0., 0., 1., 2., 2., 1., 2., 2., 0., 1., 1., 2.,\n",
      "        1., 1., 0., 2., 0., 1., 2., 2., 2., 1., 2., 1., 2., 0., 1., 2., 1., 2.,\n",
      "        2., 2., 1., 2., 1., 1., 1., 0., 0., 1., 2., 1., 2., 2., 2., 1., 0., 2.,\n",
      "        1., 2., 2., 2., 1., 2., 0., 0., 2., 1.])\n",
      "Iteration 32 ... time: 6.092072486877441 [s]\n",
      "    Labels: tensor([2., 0., 0., 0., 0., 0., 0., 2., 0., 2., 2., 1., 2., 2., 1., 0., 1., 1.,\n",
      "        0., 2., 2., 0., 1., 2., 0., 2., 2., 2., 2., 0., 2., 2., 0., 0., 1., 0.,\n",
      "        0., 0., 2., 2., 0., 0., 0., 2., 0., 0., 0., 2., 1., 2., 1., 1., 2., 2.,\n",
      "        1., 0., 1., 2., 0., 1., 2., 0., 1., 1.])\n",
      "Iteration 33 ... time: 4.110058069229126 [s]\n",
      "    Labels: tensor([1., 2., 1., 1., 1., 2., 0., 2., 1., 0., 0., 1., 2., 1., 2., 1., 2., 0.,\n",
      "        0., 2., 1., 2., 1., 2., 2., 0., 1., 2., 1., 2., 1., 1., 1., 2., 0., 0.,\n",
      "        2., 0., 1., 2., 1., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 0., 2., 0.,\n",
      "        2., 1., 1., 2., 2., 2., 1., 2., 0., 1.])\n",
      "Iteration 34 ... time: 0.0017781257629394531 [s]\n",
      "    Labels: tensor([0., 1., 1., 0., 1., 0., 2., 2., 2., 0., 0., 0., 2., 1., 2., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 2., 1., 2., 0., 2., 2., 0., 1., 2., 2., 0., 2., 0., 2.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 2., 2., 1., 0., 0., 0., 1., 1., 2., 0., 2.,\n",
      "        1., 0., 2., 2., 0., 0., 2., 2., 0., 1.])\n",
      "Iteration 35 ... time: 0.0017082691192626953 [s]\n",
      "    Labels: tensor([1., 0., 0., 2., 2., 0., 2., 2., 0., 1., 2., 1., 0., 2., 2., 0., 2., 2.,\n",
      "        2., 0., 1., 0., 2., 2., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        2., 0., 2., 2., 2., 0., 2., 2., 1., 0., 2., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        2., 2., 2., 2., 0., 2., 1., 2., 2., 0.])\n",
      "Iteration 36 ... time: 4.403963804244995 [s]\n",
      "    Labels: tensor([2., 0., 2., 0., 0., 0., 1., 1., 1., 2., 0., 2., 1., 2., 2., 0., 0., 1.,\n",
      "        2., 2., 1., 1., 0., 2., 2., 1., 1., 2., 1., 2., 2., 2., 2., 2., 2., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 2., 1., 1., 1., 1., 2., 1., 0., 2., 2., 0., 0.,\n",
      "        0., 1., 2., 1., 0., 0., 0., 0., 2., 0.])\n",
      "Iteration 37 ... time: 7.767576694488525 [s]\n",
      "    Labels: tensor([0., 0., 1., 2., 1., 2., 0., 0., 1., 0., 1., 0., 2., 2., 2., 1., 0., 2.,\n",
      "        1., 0., 0., 1., 1., 2., 0., 2., 1., 0., 1., 0., 2., 1., 2., 2., 0., 0.,\n",
      "        1., 2., 0., 1., 2., 0., 0., 1., 0., 2., 2., 0., 0., 1., 1., 2., 1., 1.,\n",
      "        1., 1., 1., 2., 0., 1., 0., 2., 2., 0.])\n",
      "Iteration 38 ... time: 0.0018229484558105469 [s]\n",
      "    Labels: tensor([0., 0., 0., 2., 0., 1., 2., 0., 0., 1., 0., 2., 2., 0., 2., 0., 0., 1.,\n",
      "        2., 1., 2., 0., 1., 1., 2., 0., 2., 1., 0., 2., 2., 1., 1., 2., 2., 2.,\n",
      "        2., 2., 1., 2., 2., 0., 2., 0., 0., 1., 0., 0., 0., 1., 2., 2., 1., 0.,\n",
      "        2., 1., 1., 2., 2., 1., 1., 2., 2., 0.])\n",
      "Iteration 39 ... time: 0.0017850399017333984 [s]\n",
      "    Labels: tensor([2., 2., 2., 2., 2., 1., 1., 2., 2., 0., 2., 1., 0., 1., 2., 0., 0., 0.,\n",
      "        0., 2., 2., 0., 2., 2., 1., 1., 2., 0., 1., 2., 1., 1., 0., 2., 0., 2.,\n",
      "        0., 1., 2., 0., 1., 0., 0., 2., 0., 0., 1., 0., 2., 0., 1., 1., 0., 0.,\n",
      "        2., 0., 2., 1., 0., 2., 2., 2., 1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.34990568e+01, 1.98249817e-01, 4.16111946e-03, 4.15825844e-03,\n",
       "       1.41849785e+01, 1.95436168e+00, 5.96785545e-03, 5.82909584e-03,\n",
       "       1.13512354e+01, 1.92124510e+00, 4.17447090e-03, 3.78942490e-03,\n",
       "       8.56816006e+00, 4.60999870e+00, 4.51564789e-03, 4.34207916e-03,\n",
       "       7.56507301e+00, 4.99260283e+00, 4.24933434e-03, 5.04899025e-03,\n",
       "       8.09719849e+00, 3.27507114e+00, 5.53560257e-03, 5.24926186e-03,\n",
       "       1.06406584e+01, 2.01308632e+00, 5.06830215e-03, 5.02443314e-03,\n",
       "       9.58063221e+00, 7.71890545e+00, 5.58161736e-03, 5.45787811e-03,\n",
       "       6.09541368e+00, 4.11417770e+00, 5.22494316e-03, 5.37395477e-03,\n",
       "       4.40862370e+00, 7.77126789e+00, 5.00988960e-03, 4.82296944e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_over_set(train_iter, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OMG that was slow... and we aren't even oing anything with the data- do you notice that roughly every 4th iteration the time it takes to give a batch is huge? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch manager (`SLURM` on Compute Canada) gives us a location of fast storage we are supposed to use for accessing repeatedly a dataset with low latency. Any system ought to have a local fast scratch for purposes like this. Different batch managers may or may not give you a specific location for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/localscratch/wfedorko.21363245.0\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SLURM_TMPDIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the dataset there and repeat the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/scratch/wfedorko/TRIUMF_DS_NUPRISM/merged_IWCDmPMT_varyE.h5' -> '/localscratch/wfedorko.21363245.0/merged_IWCDmPMT_varyE.h5'\n",
      "that took 213.89068460464478 seconds\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "!cp /scratch/wfedorko/TRIUMF_DS_NUPRISM/merged_IWCDmPMT_varyE.h5 ${SLURM_TMPDIR}/ -v\n",
    "tdiff=time.time()-t\n",
    "print(\"that took {} seconds\".format(tdiff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset=WCH5Dataset(os.environ['SLURM_TMPDIR']+\"/merged_IWCDmPMT_varyE.h5\",val_split=0.1,test_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter=DataLoader(dset,batch_size=64,shuffle=False,sampler=SubsetRandomSampler(dset.train_indices),num_workers=4)\n",
    "val_iter=DataLoader(dset,batch_size=64,shuffle=False,sampler=SubsetRandomSampler(dset.val_indices),num_workers=4)\n",
    "test_iter=DataLoader(dset,batch_size=64,shuffle=False,sampler=SubsetRandomSampler(dset.test_indices),num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 ... time: 0.39338254928588867 [s]\n",
      "    Labels: tensor([1., 1., 1., 2., 2., 1., 0., 1., 1., 0., 2., 0., 0., 1., 0., 0., 2., 1.,\n",
      "        0., 2., 1., 2., 0., 2., 0., 1., 0., 0., 1., 0., 0., 2., 0., 1., 2., 0.,\n",
      "        2., 1., 2., 0., 2., 2., 1., 0., 1., 0., 0., 0., 2., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 2., 2., 0., 2., 2., 0., 1.])\n",
      "Iteration 1 ... time: 0.006192922592163086 [s]\n",
      "    Labels: tensor([0., 2., 2., 2., 2., 1., 1., 1., 1., 1., 0., 2., 0., 0., 2., 1., 2., 2.,\n",
      "        1., 1., 2., 2., 0., 1., 1., 0., 2., 2., 0., 1., 0., 1., 1., 2., 1., 0.,\n",
      "        2., 0., 0., 1., 2., 2., 0., 0., 0., 0., 2., 1., 2., 2., 2., 0., 1., 1.,\n",
      "        1., 0., 0., 2., 2., 1., 2., 2., 2., 1.])\n",
      "Iteration 2 ... time: 0.0015833377838134766 [s]\n",
      "    Labels: tensor([2., 1., 0., 2., 1., 2., 1., 1., 2., 0., 2., 1., 2., 0., 1., 2., 0., 2.,\n",
      "        1., 1., 2., 1., 2., 2., 1., 1., 2., 1., 0., 2., 1., 0., 0., 2., 0., 2.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 2., 2., 1., 0., 2., 2., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 2., 1., 2., 1., 2., 2.])\n",
      "Iteration 3 ... time: 0.0015835762023925781 [s]\n",
      "    Labels: tensor([1., 2., 1., 0., 2., 1., 2., 1., 2., 2., 1., 0., 1., 2., 1., 2., 0., 1.,\n",
      "        1., 0., 2., 1., 0., 2., 2., 2., 0., 2., 2., 1., 1., 2., 0., 1., 1., 2.,\n",
      "        2., 1., 2., 1., 0., 1., 2., 0., 2., 1., 2., 1., 1., 2., 2., 1., 0., 2.,\n",
      "        2., 2., 1., 2., 1., 2., 2., 0., 0., 2.])\n",
      "Iteration 4 ... time: 0.12232828140258789 [s]\n",
      "    Labels: tensor([0., 2., 0., 1., 1., 0., 0., 0., 0., 1., 2., 1., 1., 0., 1., 2., 1., 1.,\n",
      "        2., 1., 2., 2., 0., 0., 1., 2., 2., 0., 1., 1., 2., 2., 2., 0., 1., 1.,\n",
      "        1., 0., 2., 1., 2., 1., 0., 0., 1., 0., 2., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        2., 0., 1., 2., 0., 0., 0., 1., 2., 2.])\n",
      "Iteration 5 ... time: 0.0015425682067871094 [s]\n",
      "    Labels: tensor([2., 2., 2., 2., 0., 0., 1., 0., 1., 1., 2., 2., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 2., 2., 2., 1., 2., 0., 1., 2., 2., 2., 2., 2., 0.,\n",
      "        1., 0., 2., 1., 1., 2., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 2.,\n",
      "        0., 2., 1., 2., 0., 0., 0., 2., 0., 1.])\n",
      "Iteration 6 ... time: 0.003675699234008789 [s]\n",
      "    Labels: tensor([1., 0., 2., 0., 1., 0., 1., 2., 1., 2., 1., 2., 0., 2., 0., 0., 1., 1.,\n",
      "        1., 2., 2., 2., 2., 0., 2., 0., 0., 2., 0., 2., 2., 0., 1., 2., 1., 1.,\n",
      "        1., 2., 1., 0., 0., 0., 1., 1., 2., 2., 0., 0., 2., 0., 0., 2., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 1., 1.])\n",
      "Iteration 7 ... time: 0.0015604496002197266 [s]\n",
      "    Labels: tensor([2., 2., 0., 0., 2., 1., 0., 1., 0., 2., 0., 1., 0., 2., 1., 1., 0., 2.,\n",
      "        1., 2., 0., 2., 2., 0., 0., 1., 0., 0., 1., 1., 0., 2., 2., 2., 1., 2.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 2., 2., 1., 0., 1., 2., 2., 1., 2., 2., 1.,\n",
      "        1., 1., 2., 2., 1., 0., 2., 2., 0., 2.])\n",
      "Iteration 8 ... time: 0.11562776565551758 [s]\n",
      "    Labels: tensor([1., 0., 1., 0., 1., 2., 1., 0., 1., 2., 2., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 2., 2., 1., 2., 1., 0., 1., 1., 2., 0.,\n",
      "        2., 1., 2., 1., 0., 1., 2., 2., 1., 2., 1., 1., 2., 2., 2., 2., 1., 0.,\n",
      "        2., 2., 1., 1., 1., 2., 0., 2., 1., 1.])\n",
      "Iteration 9 ... time: 0.007471799850463867 [s]\n",
      "    Labels: tensor([2., 0., 2., 2., 2., 1., 2., 0., 1., 2., 0., 1., 2., 0., 0., 2., 0., 2.,\n",
      "        0., 1., 0., 0., 1., 2., 1., 0., 2., 0., 2., 1., 2., 1., 2., 1., 1., 1.,\n",
      "        2., 0., 2., 0., 1., 1., 2., 1., 0., 1., 0., 1., 1., 1., 0., 0., 2., 0.,\n",
      "        1., 2., 2., 2., 1., 1., 2., 2., 2., 1.])\n",
      "Iteration 10 ... time: 0.004614114761352539 [s]\n",
      "    Labels: tensor([2., 2., 0., 2., 0., 2., 2., 2., 0., 2., 2., 0., 2., 1., 0., 0., 2., 0.,\n",
      "        0., 1., 2., 2., 0., 2., 2., 1., 0., 1., 2., 1., 1., 0., 0., 0., 2., 0.,\n",
      "        1., 0., 2., 2., 2., 0., 2., 1., 1., 1., 1., 2., 0., 1., 2., 0., 1., 2.,\n",
      "        1., 0., 2., 0., 2., 0., 2., 0., 1., 1.])\n",
      "Iteration 11 ... time: 0.0015728473663330078 [s]\n",
      "    Labels: tensor([0., 1., 1., 2., 1., 1., 1., 1., 1., 2., 2., 1., 2., 2., 2., 1., 1., 1.,\n",
      "        1., 0., 2., 2., 2., 0., 1., 0., 1., 2., 0., 1., 2., 0., 0., 0., 1., 0.,\n",
      "        1., 2., 0., 1., 2., 0., 1., 2., 2., 2., 0., 0., 2., 0., 2., 2., 2., 0.,\n",
      "        0., 0., 2., 0., 2., 1., 1., 1., 2., 1.])\n",
      "Iteration 12 ... time: 0.12614107131958008 [s]\n",
      "    Labels: tensor([1., 0., 1., 0., 0., 2., 0., 2., 0., 0., 2., 0., 1., 0., 0., 1., 0., 2.,\n",
      "        2., 1., 1., 0., 2., 0., 1., 1., 0., 2., 1., 1., 0., 0., 0., 2., 2., 1.,\n",
      "        0., 1., 2., 1., 1., 0., 0., 1., 0., 0., 1., 1., 2., 2., 1., 2., 2., 2.,\n",
      "        1., 2., 1., 1., 1., 2., 0., 0., 2., 1.])\n",
      "Iteration 13 ... time: 0.02040410041809082 [s]\n",
      "    Labels: tensor([2., 1., 0., 2., 1., 2., 1., 1., 1., 2., 0., 2., 1., 0., 2., 2., 1., 0.,\n",
      "        1., 1., 1., 2., 2., 2., 0., 1., 1., 2., 0., 1., 0., 2., 1., 0., 0., 1.,\n",
      "        2., 1., 0., 2., 0., 2., 2., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 2., 1., 0., 0., 0., 0., 0.])\n",
      "Iteration 14 ... time: 0.004637241363525391 [s]\n",
      "    Labels: tensor([0., 2., 1., 1., 0., 1., 0., 0., 0., 2., 0., 2., 1., 2., 0., 2., 1., 0.,\n",
      "        1., 1., 2., 1., 2., 0., 2., 1., 2., 0., 1., 2., 1., 0., 2., 1., 2., 2.,\n",
      "        1., 1., 2., 2., 0., 1., 1., 0., 2., 1., 2., 1., 0., 0., 2., 2., 2., 2.,\n",
      "        0., 1., 1., 2., 0., 2., 2., 0., 0., 2.])\n",
      "Iteration 15 ... time: 0.0015654563903808594 [s]\n",
      "    Labels: tensor([2., 0., 0., 1., 0., 1., 1., 2., 2., 1., 0., 0., 1., 2., 0., 1., 2., 2.,\n",
      "        2., 2., 2., 1., 1., 1., 1., 0., 2., 2., 1., 0., 1., 1., 1., 0., 2., 1.,\n",
      "        1., 1., 2., 0., 1., 1., 2., 2., 2., 2., 0., 1., 2., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 2., 0., 2., 1.])\n",
      "Iteration 16 ... time: 0.10747361183166504 [s]\n",
      "    Labels: tensor([0., 0., 0., 0., 0., 2., 0., 1., 0., 2., 1., 0., 1., 1., 1., 0., 0., 2.,\n",
      "        1., 0., 1., 1., 0., 0., 2., 1., 1., 1., 0., 2., 1., 0., 2., 0., 1., 2.,\n",
      "        2., 2., 2., 0., 0., 0., 0., 2., 1., 1., 0., 0., 0., 0., 2., 1., 2., 1.,\n",
      "        0., 2., 2., 1., 2., 1., 2., 0., 2., 2.])\n",
      "Iteration 17 ... time: 0.026961326599121094 [s]\n",
      "    Labels: tensor([0., 2., 1., 0., 2., 2., 2., 2., 0., 0., 2., 0., 0., 1., 2., 2., 0., 2.,\n",
      "        0., 1., 1., 0., 0., 1., 2., 1., 1., 0., 0., 1., 1., 0., 2., 1., 2., 1.,\n",
      "        2., 2., 0., 2., 1., 1., 2., 1., 1., 2., 1., 1., 0., 0., 1., 2., 2., 2.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
      "Iteration 18 ... time: 0.002665281295776367 [s]\n",
      "    Labels: tensor([1., 0., 1., 0., 2., 0., 0., 2., 1., 1., 2., 2., 1., 0., 2., 2., 0., 1.,\n",
      "        2., 0., 2., 0., 1., 1., 2., 2., 0., 2., 0., 2., 0., 1., 1., 1., 0., 2.,\n",
      "        2., 0., 0., 0., 1., 0., 1., 1., 2., 2., 2., 2., 0., 2., 2., 0., 0., 1.,\n",
      "        1., 2., 2., 0., 0., 2., 0., 0., 0., 2.])\n",
      "Iteration 19 ... time: 0.0013341903686523438 [s]\n",
      "    Labels: tensor([1., 0., 0., 2., 0., 0., 2., 0., 0., 1., 1., 0., 2., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 2., 2., 0., 2., 2., 2., 1., 1., 2., 2., 2., 1., 0., 1., 2.,\n",
      "        1., 2., 1., 2., 0., 1., 1., 1., 2., 2., 1., 2., 2., 1., 1., 1., 1., 1.,\n",
      "        0., 2., 2., 0., 2., 0., 1., 2., 2., 2.])\n",
      "Iteration 20 ... time: 0.10456180572509766 [s]\n",
      "    Labels: tensor([2., 1., 2., 0., 1., 1., 2., 2., 0., 2., 2., 2., 1., 0., 2., 1., 0., 2.,\n",
      "        1., 1., 1., 2., 2., 0., 2., 2., 2., 1., 2., 1., 0., 0., 2., 1., 0., 1.,\n",
      "        2., 1., 0., 0., 1., 2., 1., 1., 0., 0., 2., 0., 2., 0., 2., 2., 0., 2.,\n",
      "        2., 1., 2., 2., 2., 0., 1., 2., 2., 1.])\n",
      "Iteration 21 ... time: 0.029560565948486328 [s]\n",
      "    Labels: tensor([0., 0., 1., 1., 2., 2., 0., 2., 0., 1., 1., 2., 2., 2., 2., 2., 2., 1.,\n",
      "        1., 0., 2., 1., 1., 1., 1., 2., 1., 2., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 2., 2., 1., 2., 0., 2., 1., 0., 1., 1., 0., 0.,\n",
      "        2., 0., 0., 0., 1., 2., 2., 2., 2., 0.])\n",
      "Iteration 22 ... time: 0.00418543815612793 [s]\n",
      "    Labels: tensor([1., 1., 0., 0., 1., 0., 0., 1., 1., 2., 2., 1., 1., 2., 1., 1., 2., 1.,\n",
      "        1., 2., 2., 0., 2., 0., 1., 1., 1., 1., 0., 1., 1., 2., 1., 0., 2., 1.,\n",
      "        1., 2., 2., 0., 1., 0., 0., 2., 0., 1., 0., 1., 1., 1., 0., 2., 0., 2.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 1., 2.])\n",
      "Iteration 23 ... time: 0.0015497207641601562 [s]\n",
      "    Labels: tensor([0., 0., 0., 0., 1., 2., 1., 1., 2., 0., 1., 2., 0., 2., 2., 2., 0., 0.,\n",
      "        1., 2., 1., 1., 0., 1., 1., 2., 2., 0., 0., 1., 1., 0., 0., 2., 1., 0.,\n",
      "        0., 0., 2., 1., 0., 0., 1., 0., 2., 1., 2., 0., 1., 0., 2., 0., 0., 0.,\n",
      "        1., 2., 1., 1., 1., 2., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24 ... time: 0.10514974594116211 [s]\n",
      "    Labels: tensor([2., 0., 0., 2., 0., 1., 2., 2., 1., 2., 0., 1., 1., 2., 0., 0., 2., 0.,\n",
      "        2., 0., 0., 0., 1., 2., 1., 2., 2., 1., 1., 1., 2., 0., 2., 1., 0., 2.,\n",
      "        1., 0., 2., 1., 1., 2., 0., 1., 2., 1., 1., 2., 0., 2., 2., 1., 2., 0.,\n",
      "        2., 1., 0., 2., 1., 1., 1., 1., 0., 0.])\n",
      "Iteration 25 ... time: 0.031655311584472656 [s]\n",
      "    Labels: tensor([2., 2., 1., 1., 2., 0., 0., 0., 0., 2., 2., 1., 2., 0., 1., 0., 2., 2.,\n",
      "        0., 0., 2., 2., 0., 2., 1., 1., 1., 2., 0., 0., 0., 2., 2., 2., 2., 2.,\n",
      "        2., 1., 0., 2., 1., 1., 0., 1., 2., 1., 0., 2., 0., 0., 2., 1., 0., 0.,\n",
      "        1., 0., 1., 2., 1., 2., 1., 2., 0., 0.])\n",
      "Iteration 26 ... time: 0.0014500617980957031 [s]\n",
      "    Labels: tensor([2., 2., 0., 2., 0., 0., 2., 2., 0., 2., 1., 2., 1., 1., 2., 2., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 2., 2., 2., 0., 1., 1., 0., 1., 1., 0., 1., 2.,\n",
      "        2., 2., 1., 1., 2., 1., 1., 2., 0., 0., 1., 2., 0., 0., 1., 0., 1., 2.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 2., 0., 2.])\n",
      "Iteration 27 ... time: 0.0015132427215576172 [s]\n",
      "    Labels: tensor([2., 1., 1., 2., 0., 1., 2., 0., 0., 0., 1., 0., 1., 2., 2., 0., 0., 2.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 2., 2., 0., 1.,\n",
      "        2., 0., 1., 1., 2., 2., 0., 2., 2., 0., 0., 0., 0., 0., 0., 2., 2., 2.,\n",
      "        1., 1., 2., 0., 1., 0., 1., 2., 1., 2.])\n",
      "Iteration 28 ... time: 0.10300135612487793 [s]\n",
      "    Labels: tensor([2., 2., 0., 1., 2., 2., 0., 0., 2., 1., 2., 2., 0., 0., 1., 0., 0., 1.,\n",
      "        2., 0., 1., 0., 1., 2., 1., 2., 2., 2., 0., 1., 2., 1., 2., 1., 0., 1.,\n",
      "        2., 2., 1., 1., 2., 1., 1., 1., 0., 2., 1., 2., 0., 1., 2., 1., 0., 0.,\n",
      "        1., 2., 2., 0., 1., 2., 2., 1., 0., 1.])\n",
      "Iteration 29 ... time: 0.03930544853210449 [s]\n",
      "    Labels: tensor([2., 2., 2., 2., 1., 1., 0., 1., 1., 0., 1., 0., 0., 2., 0., 1., 2., 2.,\n",
      "        2., 1., 0., 0., 2., 0., 2., 0., 1., 0., 0., 2., 1., 2., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 2., 0., 2., 1., 0., 1., 2., 2., 1., 2., 2., 2.,\n",
      "        0., 0., 1., 2., 0., 0., 1., 2., 1., 1.])\n",
      "Iteration 30 ... time: 0.0014123916625976562 [s]\n",
      "    Labels: tensor([0., 0., 2., 2., 2., 2., 2., 2., 2., 0., 2., 1., 0., 0., 0., 0., 2., 0.,\n",
      "        2., 2., 2., 1., 1., 2., 0., 2., 2., 0., 0., 0., 2., 0., 0., 1., 0., 2.,\n",
      "        1., 0., 2., 1., 1., 0., 1., 0., 2., 1., 2., 1., 0., 1., 1., 2., 0., 0.,\n",
      "        0., 0., 2., 0., 0., 2., 2., 1., 0., 0.])\n",
      "Iteration 31 ... time: 0.0015625953674316406 [s]\n",
      "    Labels: tensor([0., 2., 2., 2., 0., 0., 2., 0., 0., 0., 1., 0., 1., 1., 2., 1., 0., 1.,\n",
      "        2., 2., 1., 2., 1., 2., 2., 1., 1., 0., 1., 2., 2., 1., 2., 0., 1., 1.,\n",
      "        1., 0., 0., 2., 0., 1., 1., 2., 1., 2., 1., 1., 0., 0., 1., 0., 0., 2.,\n",
      "        1., 2., 1., 2., 1., 1., 1., 0., 0., 0.])\n",
      "Iteration 32 ... time: 0.10083794593811035 [s]\n",
      "    Labels: tensor([2., 1., 0., 2., 1., 2., 1., 1., 0., 2., 2., 1., 1., 1., 1., 2., 2., 2.,\n",
      "        2., 0., 0., 0., 2., 1., 0., 1., 0., 2., 2., 1., 1., 2., 0., 2., 2., 2.,\n",
      "        1., 2., 2., 1., 2., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 2., 2., 0.,\n",
      "        0., 0., 2., 1., 1., 0., 1., 0., 0., 2.])\n",
      "Iteration 33 ... time: 0.027446269989013672 [s]\n",
      "    Labels: tensor([2., 2., 2., 0., 0., 2., 0., 2., 0., 2., 1., 2., 0., 0., 0., 1., 1., 2.,\n",
      "        0., 0., 1., 0., 2., 0., 1., 2., 0., 1., 0., 1., 0., 2., 0., 1., 2., 0.,\n",
      "        0., 0., 0., 2., 2., 0., 0., 2., 1., 0., 2., 1., 0., 2., 2., 1., 1., 2.,\n",
      "        2., 1., 0., 1., 1., 1., 1., 0., 2., 2.])\n",
      "Iteration 34 ... time: 0.0017421245574951172 [s]\n",
      "    Labels: tensor([2., 1., 2., 2., 1., 0., 0., 1., 1., 0., 0., 2., 0., 1., 2., 1., 1., 0.,\n",
      "        2., 0., 2., 1., 1., 1., 2., 2., 2., 2., 0., 1., 1., 1., 2., 0., 2., 1.,\n",
      "        0., 0., 1., 1., 0., 2., 0., 2., 2., 0., 0., 1., 2., 0., 1., 1., 2., 2.,\n",
      "        0., 2., 2., 0., 2., 2., 0., 0., 1., 0.])\n",
      "Iteration 35 ... time: 0.0014324188232421875 [s]\n",
      "    Labels: tensor([1., 0., 1., 0., 2., 0., 1., 1., 0., 1., 0., 2., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 2., 0., 0., 2., 2., 1., 0., 0., 2., 2., 2., 0., 0., 0., 1., 0., 2.,\n",
      "        0., 0., 0., 0., 2., 2., 0., 0., 1., 0., 0., 0., 0., 2., 2., 0., 0., 0.,\n",
      "        1., 1., 2., 1., 0., 0., 0., 0., 1., 2.])\n",
      "Iteration 36 ... time: 0.09908771514892578 [s]\n",
      "    Labels: tensor([1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 2., 1., 2., 2., 2., 1., 0.,\n",
      "        2., 2., 0., 2., 1., 1., 0., 0., 1., 0., 0., 2., 2., 2., 0., 2., 1., 0.,\n",
      "        0., 0., 0., 2., 1., 2., 1., 1., 1., 2., 0., 0., 0., 1., 2., 1., 1., 1.,\n",
      "        2., 2., 2., 2., 2., 1., 0., 2., 2., 0.])\n",
      "Iteration 37 ... time: 0.03413891792297363 [s]\n",
      "    Labels: tensor([2., 2., 2., 2., 1., 0., 1., 0., 1., 0., 0., 2., 2., 1., 0., 2., 1., 1.,\n",
      "        2., 2., 2., 0., 1., 0., 0., 2., 1., 1., 2., 0., 0., 2., 1., 1., 0., 1.,\n",
      "        2., 0., 0., 0., 1., 2., 0., 2., 0., 0., 0., 2., 2., 0., 1., 0., 2., 1.,\n",
      "        2., 0., 2., 0., 1., 0., 2., 2., 1., 1.])\n",
      "Iteration 38 ... time: 0.0014564990997314453 [s]\n",
      "    Labels: tensor([2., 2., 2., 2., 0., 0., 2., 2., 0., 0., 0., 1., 0., 1., 2., 0., 2., 0.,\n",
      "        2., 1., 2., 0., 0., 0., 2., 1., 2., 0., 1., 1., 0., 2., 2., 0., 0., 2.,\n",
      "        0., 2., 0., 1., 0., 2., 0., 2., 1., 2., 0., 1., 2., 0., 2., 0., 1., 0.,\n",
      "        1., 2., 2., 2., 0., 0., 1., 2., 1., 2.])\n",
      "Iteration 39 ... time: 0.0016117095947265625 [s]\n",
      "    Labels: tensor([1., 1., 0., 2., 1., 1., 0., 1., 2., 1., 2., 1., 1., 1., 1., 2., 1., 0.,\n",
      "        2., 0., 1., 2., 0., 1., 2., 1., 0., 1., 1., 1., 2., 2., 1., 2., 1., 1.,\n",
      "        2., 0., 2., 2., 0., 0., 1., 2., 0., 0., 2., 1., 2., 1., 0., 0., 0., 0.,\n",
      "        2., 2., 0., 0., 2., 1., 1., 1., 0., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.3971002 , 0.00924706, 0.00454521, 0.00485611, 0.1251483 ,\n",
       "       0.00410581, 0.00636053, 0.00424099, 0.11881042, 0.0102489 ,\n",
       "       0.00733089, 0.0042491 , 0.12887645, 0.02322483, 0.00766492,\n",
       "       0.00466919, 0.11041856, 0.03000998, 0.00548697, 0.00416946,\n",
       "       0.10714102, 0.03271365, 0.00690269, 0.00406194, 0.10816264,\n",
       "       0.03446817, 0.00417995, 0.0039587 , 0.10596228, 0.04212618,\n",
       "       0.00401545, 0.00505567, 0.10406089, 0.03024292, 0.00436854,\n",
       "       0.00369549, 0.10205698, 0.03704715, 0.00408864, 0.0042901 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_over_set(train_iter, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok - way faster - even though it took a long time initially to copy the dataset - it's going to remove the bottleneck of data access during training - especially that we are going to be looping over the dataset many times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to get familiar with the data. Plot different event in different classes. Energy sums and histograms. Time histograms etc. Can you 'spot' the differences between classes 'by eye'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And remember to clean up!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed '/localscratch/wfedorko.21363245.0/merged_IWCDmPMT_varyE.h5'\r\n",
      "removed '/localscratch/wfedorko.21363245.0/test'\r\n"
     ]
    }
   ],
   "source": [
    "!rm -v ${SLURM_TMPDIR}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
